06-07 01:52:40: Training starting...
06-07 01:52:42: Reward -12.752 at episode 0, saving model...
06-07 01:52:42: Reward -12.549 at episode 1, saving model...
06-07 01:52:42: Reward -10.288 at episode 2, saving model...
06-07 01:52:42: Reward -10.111 at episode 3, saving model...
06-07 01:52:42: Reward 10.172 at episode 9, saving model...
06-07 01:52:42: Reward 10.432 at episode 12, saving model...
06-07 01:52:42: Reward 14.169 at episode 22, saving model...
06-07 01:52:45: Reward 14.291 at episode 106, saving model...
06-07 01:52:51: Reward 14.701 at episode 425, saving model...
06-07 01:52:59: Reward 14.731 at episode 1127, saving model...
06-07 01:53:27: Reward 14.971 at episode 3374, saving model...
06-07 01:53:48: Reward -12.288 at episode 5000, saving model...
06-07 01:54:22: Reward 15.000 at episode 7684, saving model...
06-07 01:54:53: Reward 12.240 at episode 10000, saving model...
06-07 01:55:59: Reward -12.472 at episode 15000, saving model...
06-07 01:57:06: Reward -12.497 at episode 20000, saving model...
06-07 01:58:14: Reward -12.458 at episode 25000, saving model...
06-07 01:59:25: Reward -12.283 at episode 30000, saving model...
06-07 02:00:35: Reward 11.242 at episode 35000, saving model...
06-07 02:01:46: Reward 10.215 at episode 40000, saving model...
06-07 02:02:56: Reward -10.098 at episode 45000, saving model...
06-07 02:04:09: Reward -10.420 at episode 50000, saving model...
06-07 02:05:20: Reward -10.073 at episode 55000, saving model...
06-07 02:06:32: Reward 10.161 at episode 60000, saving model...
06-07 02:07:46: Reward 11.112 at episode 65000, saving model...
06-07 02:09:01: Reward 12.554 at episode 70000, saving model...
06-07 02:10:16: Reward -12.624 at episode 75000, saving model...
06-07 02:11:32: Reward 12.826 at episode 80000, saving model...
06-07 02:12:50: Reward 10.264 at episode 85000, saving model...
06-07 02:14:08: Reward -12.931 at episode 90000, saving model...
06-07 02:15:27: Reward 12.097 at episode 95000, saving model...
06-07 02:16:47: Reward -10.416 at episode 100000, saving model...
06-07 02:18:10: Reward 11.375 at episode 105000, saving model...
06-07 02:19:30: Reward -10.031 at episode 110000, saving model...
06-07 02:20:52: Reward -12.364 at episode 115000, saving model...
06-07 02:22:18: Reward 10.141 at episode 120000, saving model...
06-07 02:23:43: Reward -10.022 at episode 125000, saving model...
06-07 02:25:11: Reward 12.464 at episode 130000, saving model...
06-07 02:26:39: Reward 10.030 at episode 135000, saving model...
06-07 02:28:12: Reward -12.475 at episode 140000, saving model...
06-07 02:29:45: Reward -12.962 at episode 145000, saving model...
06-07 02:31:20: Reward -12.278 at episode 150000, saving model...
06-07 02:32:57: Reward -12.804 at episode 155000, saving model...
06-07 02:34:36: Reward -12.462 at episode 160000, saving model...
06-07 02:36:13: Reward -12.803 at episode 165000, saving model...
06-07 02:37:53: Reward 10.473 at episode 170000, saving model...
06-07 02:39:37: Reward -10.231 at episode 175000, saving model...
06-07 02:41:21: Reward -11.591 at episode 180000, saving model...
06-07 02:43:08: Reward 10.362 at episode 185000, saving model...
06-07 02:44:57: Reward -12.809 at episode 190000, saving model...
06-07 02:46:45: Reward -10.034 at episode 195000, saving model...
06-07 02:48:35: Reward -12.580 at episode 200000, saving model...
06-07 02:50:27: Reward 10.426 at episode 205000, saving model...
06-07 02:52:23: Reward -12.421 at episode 210000, saving model...
06-07 02:54:22: Reward -12.905 at episode 215000, saving model...
06-07 02:56:23: Reward 10.179 at episode 220000, saving model...
06-07 02:58:24: Reward -11.465 at episode 225000, saving model...
06-07 03:00:28: Reward -12.591 at episode 230000, saving model...
06-07 03:02:36: Reward -10.222 at episode 235000, saving model...
06-07 03:04:45: Reward -12.344 at episode 240000, saving model...
06-07 03:06:57: Reward -12.588 at episode 245000, saving model...
06-07 03:09:13: Reward 10.460 at episode 250000, saving model...
06-07 03:11:33: Reward 10.391 at episode 255000, saving model...
06-07 03:13:57: Reward -12.359 at episode 260000, saving model...
06-07 03:16:22: Reward -10.347 at episode 265000, saving model...
06-07 03:18:53: Reward 13.412 at episode 270000, saving model...
06-07 03:21:23: Reward -12.569 at episode 275000, saving model...
06-07 03:24:07: Reward -12.624 at episode 280000, saving model...
06-07 03:26:51: Reward -10.065 at episode 285000, saving model...
06-07 03:29:40: Reward -12.365 at episode 290000, saving model...
06-07 03:32:32: Reward -12.460 at episode 295000, saving model...
06-07 03:35:28: Reward -10.250 at episode 300000, saving model...
06-07 03:38:32: Reward -12.367 at episode 305000, saving model...
06-07 03:41:41: Reward -12.360 at episode 310000, saving model...
06-07 03:44:57: Reward -12.925 at episode 315000, saving model...
06-07 03:48:21: Reward -12.434 at episode 320000, saving model...
06-07 03:51:50: Reward 10.226 at episode 325000, saving model...
06-07 03:55:22: Reward -12.829 at episode 330000, saving model...
06-07 03:59:03: Reward -12.617 at episode 335000, saving model...
06-07 04:02:59: Reward 12.608 at episode 340000, saving model...
06-07 04:06:57: Reward 11.267 at episode 345000, saving model...
06-07 04:11:13: Reward -10.046 at episode 350000, saving model...
06-07 04:15:38: Reward -10.068 at episode 355000, saving model...
06-07 04:20:08: Reward -12.957 at episode 360000, saving model...
06-07 04:25:00: Reward -12.675 at episode 365000, saving model...
06-07 04:30:05: Reward -12.702 at episode 370000, saving model...
06-07 04:35:29: Reward 10.152 at episode 375000, saving model...
06-07 04:41:06: Reward -10.105 at episode 380000, saving model...
06-07 04:47:06: Reward 12.591 at episode 385000, saving model...
06-07 04:53:32: Reward 10.100 at episode 390000, saving model...
06-07 05:00:25: Reward -12.265 at episode 395000, saving model...
06-07 05:07:55: Reward -12.365 at episode 400000, saving model...
06-07 05:15:45: Reward 10.394 at episode 405000, saving model...
06-07 05:24:27: Reward -12.819 at episode 410000, saving model...
06-07 05:34:09: Reward -12.538 at episode 415000, saving model...
06-07 05:45:24: Reward -12.870 at episode 420000, saving model...
06-07 05:58:25: Reward -12.887 at episode 425000, saving model...
06-07 06:13:07: Reward -10.305 at episode 430000, saving model...
06-07 06:30:01: Reward 10.282 at episode 435000, saving model...
06-07 06:51:01: Reward 5.205 at episode 440000, saving model...
06-07 07:16:36: Reward -10.028 at episode 445000, saving model...
06-07 07:49:31: Reward -10.325 at episode 450000, saving model...

pokemon_battle:
  env_id: pokemon_battle_env
  replay_memory_size: 100000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.999995
  epsilon_min: 0.05
  network_sync_rate: 8000
  learning_rate_a: 0.0001
  discount_factor_g: 0.99
  stop_on_reward: 100000
  train_freq: 4
  enable_double_dqn: True
  enable_dueling_dqn: True

  import torch
from torch import nn
import torch.nn.functional as F
from move_encoder import MoveEmbedder
from custom_encodings import ENCODING_CONSTANTS

class DQN(nn.Module):

    def __init__(self, action_dim=64, enable_dueling_dqn=True):
        super().__init__()
        self.n_moves    = ENCODING_CONSTANTS.MAX_MOVES*ENCODING_CONSTANTS.MAX_PKM_PER_TEAM + ENCODING_CONSTANTS.MAX_PKM_PER_TEAM
        self.embed_dim  = ENCODING_CONSTANTS.EMBEDDED_MOVE
        self.field_dim  = ENCODING_CONSTANTS.STATE-ENCODING_CONSTANTS.MOVE*self.n_moves
        self.enable_dueling_dqn = enable_dueling_dqn

        # Initialize move embedder
        self.move_embedder = MoveEmbedder()

        # Hidden layer dim
        self.hl1_dim = 256
        self.hl2_dim = 128

        # DQN head now takes aggregated field and embedded move features
        self.fc1 = nn.Linear(self.field_dim + self.n_moves * ENCODING_CONSTANTS.EMBEDDED_MOVE, self.hl1_dim)
        self.fc2 = nn.Linear(self.hl1_dim, self.hl2_dim)

        if self.enable_dueling_dqn:
            # Value stream
            self.fc_value = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.value = nn.Linear(self.hl2_dim, 1)

            # Advantage stream
            self.fc_advantages = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.advantages = nn.Linear(self.hl2_dim, action_dim)
        else:
            self.output_layer = nn.Linear(self.hl2_dim, action_dim)

    def forward(self, x):
        # x is (batch, state features + move features)
        field_state = x[:, :self.field_dim]
        
        # Handle move encoding
        moves_raw   = x[:, self.field_dim:]
        moves_raw = moves_raw.reshape(-1, self.n_moves, ENCODING_CONSTANTS.MOVE)
        moves_emb = self.move_embedder(moves_raw)
        moves_emb = moves_emb.reshape(-1, self.n_moves * self.embed_dim)

        # concat with field_state -> (batch, len(field features) + len(embedded move features))
        fused = torch.cat([field_state, moves_emb], dim=1)

        h = F.relu(self.fc1(fused))
        h = F.relu(self.fc2(h))

        # pass through DQN head
        if self.enable_dueling_dqn:
            # Value calc
            v = F.relu(self.fc_value(h))
            V = self.value(v)

            # Advantage calc
            a = F.relu(self.fc_advantages(h))
            A = self.advantages(a)

            # Calc Q
            Q = V + A - torch.mean(A, dim=1, keepdim=True)
        else:
            Q = self.output_layer(h) 

        return Q