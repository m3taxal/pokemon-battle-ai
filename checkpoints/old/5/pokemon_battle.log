06-07 11:29:13: Training starting...
06-07 11:29:13: Reward 11.109 at episode 0, saving model...
06-07 11:29:13: Reward 12.348 at episode 5, saving model...
06-07 11:29:13: Reward 12.623 at episode 9, saving model...
06-07 11:29:14: Reward 13.497 at episode 36, saving model...
06-07 11:29:14: Reward 13.614 at episode 60, saving model...
06-07 11:29:17: Reward 14.613 at episode 316, saving model...
06-07 11:29:29: Reward 14.743 at episode 1286, saving model...
06-07 11:29:44: Reward 14.751 at episode 2509, saving model...
06-07 11:30:03: Reward 14.764 at episode 4038, saving model...
06-07 11:30:15: Reward 10.454 at episode 5000, saving model...
06-07 11:30:41: Reward 15.000 at episode 7080, saving model...
06-07 11:31:19: Reward 12.513 at episode 10000, saving model...
06-07 11:32:23: Reward -12.385 at episode 15000, saving model...
06-07 11:33:30: Reward 10.380 at episode 20000, saving model...
06-07 11:34:36: Reward 10.009 at episode 25000, saving model...
06-07 11:35:43: Reward -12.405 at episode 30000, saving model...
06-07 11:36:52: Reward -13.000 at episode 35000, saving model...
06-07 11:38:01: Reward -10.377 at episode 40000, saving model...
06-07 11:39:12: Reward -12.833 at episode 45000, saving model...
06-07 11:40:23: Reward 12.364 at episode 50000, saving model...
06-07 11:41:34: Reward -12.639 at episode 55000, saving model...
06-07 11:42:47: Reward -12.710 at episode 60000, saving model...
06-07 11:44:00: Reward 10.049 at episode 65000, saving model...
06-07 11:45:15: Reward 12.496 at episode 70000, saving model...
06-07 11:46:32: Reward 10.166 at episode 75000, saving model...
06-07 11:47:49: Reward 10.022 at episode 80000, saving model...
06-07 11:49:07: Reward -10.166 at episode 85000, saving model...
06-07 11:50:25: Reward 10.204 at episode 90000, saving model...
06-07 11:51:44: Reward -12.470 at episode 95000, saving model...
06-07 11:53:04: Reward -10.223 at episode 100000, saving model...
06-07 11:54:23: Reward 10.442 at episode 105000, saving model...
06-07 11:55:46: Reward 10.227 at episode 110000, saving model...
06-07 11:57:09: Reward 10.113 at episode 115000, saving model...
06-07 11:58:34: Reward 10.139 at episode 120000, saving model...
06-07 11:59:59: Reward 10.225 at episode 125000, saving model...
06-07 12:01:26: Reward 11.425 at episode 130000, saving model...
06-07 12:02:52: Reward 10.337 at episode 135000, saving model...
06-07 12:04:22: Reward -12.344 at episode 140000, saving model...
06-07 12:05:51: Reward -12.570 at episode 145000, saving model...
06-07 12:07:24: Reward -12.226 at episode 150000, saving model...
06-07 12:09:00: Reward 12.427 at episode 155000, saving model...
06-07 12:10:36: Reward 12.603 at episode 160000, saving model...
06-07 12:12:15: Reward -10.093 at episode 165000, saving model...
06-07 12:13:56: Reward 10.286 at episode 170000, saving model...
06-07 12:15:37: Reward 10.306 at episode 175000, saving model...
06-07 12:17:20: Reward -10.340 at episode 180000, saving model...
06-07 12:19:05: Reward -11.254 at episode 185000, saving model...
06-07 12:20:54: Reward -12.890 at episode 190000, saving model...
06-07 12:22:42: Reward -10.268 at episode 195000, saving model...
06-07 12:24:34: Reward -12.757 at episode 200000, saving model...
06-07 12:26:30: Reward -10.008 at episode 205000, saving model...
06-07 12:28:25: Reward 10.386 at episode 210000, saving model...
06-07 12:30:22: Reward -10.592 at episode 215000, saving model...
06-07 12:32:22: Reward -12.666 at episode 220000, saving model...
06-07 12:34:23: Reward -12.458 at episode 225000, saving model...
06-07 12:36:27: Reward -12.870 at episode 230000, saving model...
06-07 12:38:36: Reward 10.511 at episode 235000, saving model...
06-07 12:40:45: Reward 10.231 at episode 240000, saving model...
06-07 12:42:59: Reward -10.056 at episode 245000, saving model...
06-07 12:45:13: Reward 10.345 at episode 250000, saving model...
06-07 12:47:36: Reward -10.019 at episode 255000, saving model...
06-07 12:50:01: Reward 10.095 at episode 260000, saving model...
06-07 12:52:29: Reward -12.449 at episode 265000, saving model...
06-07 12:54:58: Reward -12.732 at episode 270000, saving model...
06-07 12:57:33: Reward -12.613 at episode 275000, saving model...
06-07 13:00:10: Reward 13.654 at episode 280000, saving model...
06-07 13:02:54: Reward -9.224 at episode 285000, saving model...
06-07 13:05:42: Reward -12.793 at episode 290000, saving model...
06-07 13:08:34: Reward 10.256 at episode 295000, saving model...
06-07 13:11:35: Reward 10.350 at episode 300000, saving model...
06-07 13:14:39: Reward -12.326 at episode 305000, saving model...
06-07 13:17:44: Reward -10.012 at episode 310000, saving model...
06-07 13:21:00: Reward -10.442 at episode 315000, saving model...
06-07 13:24:21: Reward -12.190 at episode 320000, saving model...
06-07 13:27:50: Reward -13.000 at episode 325000, saving model...
06-07 13:31:26: Reward 10.475 at episode 330000, saving model...
06-07 13:35:10: Reward 12.613 at episode 335000, saving model...
06-07 13:39:07: Reward -10.338 at episode 340000, saving model...
06-07 13:43:15: Reward -12.631 at episode 345000, saving model...
06-07 13:47:40: Reward 12.408 at episode 350000, saving model...
06-07 13:52:12: Reward -12.643 at episode 355000, saving model...
06-07 13:56:46: Reward -12.763 at episode 360000, saving model...
06-07 14:01:38: Reward 12.372 at episode 365000, saving model...
06-07 14:06:43: Reward -10.050 at episode 370000, saving model...
06-07 14:12:12: Reward -10.431 at episode 375000, saving model...
06-07 14:18:00: Reward 10.183 at episode 380000, saving model...
06-07 14:24:09: Reward -10.019 at episode 385000, saving model...
06-07 14:30:47: Reward 10.387 at episode 390000, saving model...
06-07 14:37:52: Reward -12.527 at episode 395000, saving model...

pokemon_battle:
  env_id: pokemon_battle_env
  replay_memory_size: 100000
  mini_batch_size: 32
  epsilon_init: 0.3
  epsilon_decay: 0.999995
  epsilon_min: 0.01
  network_sync_rate: 8000
  learning_rate_a: 0.0001
  discount_factor_g: 0.99
  stop_on_reward: 100000
  train_freq: 4
  repeat_epsilon: True
  randomize_enemy: False
  enable_double_dqn: True
  enable_dueling_dqn: True

class DQN(nn.Module):

    def __init__(self, action_dim=64, enable_dueling_dqn=True):
        super().__init__()
        self.n_moves    = ENCODING_CONSTANTS.MAX_MOVES*ENCODING_CONSTANTS.MAX_PKM_PER_TEAM + ENCODING_CONSTANTS.MAX_PKM_PER_TEAM
        self.embed_dim  = ENCODING_CONSTANTS.EMBEDDED_MOVE
        self.field_dim  = ENCODING_CONSTANTS.STATE-ENCODING_CONSTANTS.MOVE*self.n_moves
        self.enable_dueling_dqn = enable_dueling_dqn

        # Initialize move embedder
        self.move_embedder = MoveEmbedder()

        # Hidden layer dim
        self.hl1_dim = 256
        self.hl2_dim = 128

        # DQN head now takes aggregated field and embedded move features
        self.fc1 = nn.Linear(self.field_dim + self.n_moves * ENCODING_CONSTANTS.EMBEDDED_MOVE, self.hl1_dim)
        self.fc2 = nn.Linear(self.hl1_dim, self.hl2_dim)

        if self.enable_dueling_dqn:
            # Value stream
            self.fc_value = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.value = nn.Linear(self.hl2_dim, 1)

            # Advantage stream
            self.fc_advantages = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.advantages = nn.Linear(self.hl2_dim, action_dim)
        else:
            self.output_layer = nn.Linear(self.hl2_dim, action_dim)

    def forward(self, x):
        # x is (batch, state features + move features)
        field_state = x[:, :self.field_dim]
        
        # Handle move encoding
        moves_raw   = x[:, self.field_dim:]
        moves_raw = moves_raw.reshape(-1, self.n_moves, ENCODING_CONSTANTS.MOVE)
        moves_emb = self.move_embedder(moves_raw)
        moves_emb = moves_emb.reshape(-1, self.n_moves * self.embed_dim)

        # concat with field_state -> (batch, len(field features) + len(embedded move features))
        fused = torch.cat([field_state, moves_emb], dim=1)

        h = F.relu(self.fc1(fused))
        h = F.relu(self.fc2(h))

        # pass through DQN head
        if self.enable_dueling_dqn:
            # Value calc
            v = F.relu(self.fc_value(h))
            V = self.value(v)

            # Advantage calc
            a = F.relu(self.fc_advantages(h))
            A = self.advantages(a)

            # Calc Q
            Q = V + A - torch.mean(A, dim=1, keepdim=True)
        else:
            Q = self.output_layer(h) 

        return Q