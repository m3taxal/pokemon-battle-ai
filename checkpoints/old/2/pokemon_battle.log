06-07 01:28:31: Training starting...
06-07 01:28:33: Reward 12.600 at episode 0, saving model...
06-07 01:28:33: Reward 12.689 at episode 5, saving model...
06-07 01:28:33: Reward 12.789 at episode 10, saving model...
06-07 01:28:33: Reward 13.657 at episode 20, saving model...
06-07 01:28:34: Reward 14.597 at episode 55, saving model...
06-07 01:28:37: Reward 14.911 at episode 135, saving model...
06-07 01:28:41: Reward 14.940 at episode 315, saving model...
06-07 01:28:44: Reward 15.000 at episode 565, saving model...
06-07 01:29:50: Reward -12.323 at episode 5000, saving model...
06-07 01:31:03: Reward -10.183 at episode 10000, saving model...
06-07 01:32:16: Reward 12.203 at episode 15000, saving model...
06-07 01:33:28: Reward -10.160 at episode 20000, saving model...
06-07 01:34:40: Reward 11.094 at episode 25000, saving model...
06-07 01:35:05: Reward 15.000 at episode 26718, saving model...
06-07 01:35:54: Reward 10.266 at episode 30000, saving model...
06-07 01:37:07: Reward 10.254 at episode 35000, saving model...
06-07 01:38:19: Reward -11.759 at episode 40000, saving model...
06-07 01:39:32: Reward 12.925 at episode 45000, saving model...
06-07 01:40:47: Reward 10.024 at episode 50000, saving model...
06-07 01:42:00: Reward 12.634 at episode 55000, saving model...
06-07 01:43:16: Reward 13.000 at episode 60000, saving model...
06-07 01:44:30: Reward 12.731 at episode 65000, saving model...
06-07 01:45:45: Reward 12.829 at episode 70000, saving model...
06-07 01:47:04: Reward 12.422 at episode 75000, saving model...
06-07 01:48:21: Reward -12.593 at episode 80000, saving model...
06-07 01:49:40: Reward 13.659 at episode 85000, saving model...
06-07 01:51:04: Reward -12.371 at episode 90000, saving model...

pokemon_battle:
  env_id: pokemon_battle_env
  replay_memory_size: 100000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.999995
  epsilon_min: 0.05
  network_sync_rate: 8000
  learning_rate_a: 0.0001
  discount_factor_g: 0.99
  stop_on_reward: 100000
  train_freq: 4
  enable_double_dqn: True
  enable_dueling_dqn: True

  import torch
from torch import nn
import torch.nn.functional as F
from move_encoder import MoveEmbedder
from custom_encodings import ENCODING_CONSTANTS

class DQN(nn.Module):

    def __init__(self, action_dim=64, enable_dueling_dqn=True):
        super().__init__()
        self.n_moves    = ENCODING_CONSTANTS.MAX_MOVES*ENCODING_CONSTANTS.MAX_PKM_PER_TEAM + ENCODING_CONSTANTS.MAX_PKM_PER_TEAM
        self.embed_dim  = ENCODING_CONSTANTS.EMBEDDED_MOVE
        self.field_dim  = ENCODING_CONSTANTS.STATE-ENCODING_CONSTANTS.MOVE*self.n_moves
        self.enable_dueling_dqn = enable_dueling_dqn

        # Initialize move embedder
        self.move_embedder = MoveEmbedder()

        # Hidden layer dim
        self.hl1_dim = 256
        self.hl2_dim = 128

        # DQN head now takes aggregated field and embedded move features
        self.fc1 = nn.Linear(self.field_dim + self.n_moves * ENCODING_CONSTANTS.EMBEDDED_MOVE, self.hl1_dim)
        self.fc2 = nn.Linear(self.hl1_dim, self.hl2_dim)

        if self.enable_dueling_dqn:
            # Value stream
            self.fc_value = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.value = nn.Linear(self.hl2_dim, 1)

            # Advantage stream
            self.fc_advantages = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.advantages = nn.Linear(self.hl2_dim, action_dim)
        else:
            self.output_layer = nn.Linear(self.hl2_dim, action_dim)

    def forward(self, x):
        # x is (batch, state features + move features)
        field_state = x[:, :self.field_dim]
        
        # Handle move encoding
        moves_raw   = x[:, self.field_dim:]
        moves_raw = moves_raw.reshape(-1, self.n_moves, ENCODING_CONSTANTS.MOVE)
        moves_emb = self.move_embedder(moves_raw)
        moves_emb = moves_emb.reshape(-1, self.n_moves * self.embed_dim)

        # concat with field_state -> (batch, len(field features) + len(embedded move features))
        fused = torch.cat([field_state, moves_emb], dim=1)

        h = F.relu(self.fc1(fused))
        h = F.relu(self.fc2(h))

        # pass through DQN head
        if self.enable_dueling_dqn:
            # Value calc
            v = F.relu(self.fc_value(h))
            V = self.value(v)

            # Advantage calc
            a = F.relu(self.fc_advantages(h))
            A = self.advantages(a)

            # Calc Q
            Q = V + A - torch.mean(A, dim=1, keepdim=True)
        else:
            Q = self.output_layer(h) 

        return Q