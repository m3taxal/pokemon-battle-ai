06-07 01:10:56: Training starting...
06-07 01:10:58: Reward 13.135 at episode 0, saving model...
06-07 01:10:58: Reward 14.750 at episode 3, saving model...
06-07 01:11:05: Reward 14.793 at episode 275, saving model...
06-07 01:11:05: Reward 14.956 at episode 280, saving model...
06-07 01:11:07: Reward 15.000 at episode 413, saving model...
06-07 01:12:18: Reward -12.621 at episode 5000, saving model...
06-07 01:13:39: Reward 12.475 at episode 10000, saving model...
06-07 01:14:59: Reward 10.300 at episode 15000, saving model...
06-07 01:15:35: Reward 15.000 at episode 17148, saving model...
06-07 01:16:20: Reward 10.077 at episode 20000, saving model...
06-07 01:17:41: Reward 12.688 at episode 25000, saving model...
06-07 01:19:01: Reward 12.540 at episode 30000, saving model...
06-07 01:20:19: Reward 10.134 at episode 35000, saving model...
06-07 01:21:37: Reward -12.922 at episode 40000, saving model...
06-07 01:22:57: Reward -10.136 at episode 45000, saving model...
06-07 01:24:13: Reward -10.062 at episode 50000, saving model...

pokemon_battle:
  env_id: pokemon_battle_env
  replay_memory_size: 100000
  mini_batch_size: 32
  epsilon_init: 1
  epsilon_decay: 0.999995
  epsilon_min: 0.05
  network_sync_rate: 8000
  learning_rate_a: 0.0001
  discount_factor_g: 0.99
  stop_on_reward: 100000
  train_freq: 4
  enable_double_dqn: True
  enable_dueling_dqn: True

class DQN(nn.Module):

    def __init__(self, action_dim=64, enable_dueling_dqn=True):
        super().__init__()
        self.n_moves    = ENCODING_CONSTANTS.MAX_MOVES*ENCODING_CONSTANTS.MAX_PKM_PER_TEAM + ENCODING_CONSTANTS.MAX_PKM_PER_TEAM
        self.embed_dim  = ENCODING_CONSTANTS.EMBEDDED_MOVE
        self.field_dim  = ENCODING_CONSTANTS.STATE-ENCODING_CONSTANTS.MOVE*self.n_moves
        self.enable_dueling_dqn = enable_dueling_dqn

        # Initialize move embedder
        self.move_embedder = MoveEmbedder()

        # Hidden layer dim
        self.hl1_dim = 256
        self.hl2_dim = 128

        # DQN head now takes aggregated field and embedded move features
        self.fc1 = nn.Linear(self.field_dim + self.n_moves * ENCODING_CONSTANTS.EMBEDDED_MOVE, self.hl1_dim)
        self.fc2 = nn.Linear(self.hl1_dim, self.hl2_dim)

        if self.enable_dueling_dqn:
            # Value stream
            self.fc_value = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.value = nn.Linear(self.hl2_dim, 1)

            # Advantage stream
            self.fc_advantages = nn.Linear(self.hl2_dim, self.hl2_dim)
            self.advantages = nn.Linear(self.hl2_dim, action_dim)
        else:
            self.output_layer = nn.Linear(self.hl2_dim, action_dim)

    def forward(self, x):
        # x is (batch, state features + move features)
        field_state = x[:, :self.field_dim]
        
        # Handle move encoding
        moves_raw   = x[:, self.field_dim:]
        moves_raw = moves_raw.reshape(-1, self.n_moves, ENCODING_CONSTANTS.MOVE)
        moves_emb = self.move_embedder(moves_raw)
        moves_emb = moves_emb.reshape(-1, self.n_moves * self.embed_dim)

        # concat with field_state -> (batch, len(field features) + len(embedded move features))
        fused = torch.cat([field_state, moves_emb], dim=1)

        h = F.relu(self.fc1(fused))
        h = F.relu(self.fc2(h))

        # pass through DQN head
        if self.enable_dueling_dqn:
            # Value calc
            v = F.relu(self.fc_value(h))
            V = self.value(v)

            # Advantage calc
            a = F.relu(self.fc_advantages(h))
            A = self.advantages(a)

            # Calc Q
            Q = V + A - torch.mean(A, dim=1, keepdim=True)
        else:
            Q = self.output_layer(h) 

        return Q